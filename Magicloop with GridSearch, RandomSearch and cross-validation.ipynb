{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ParameterGrid\n",
    "from sklearn import preprocessing, svm, metrics, tree, decomposition\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, y_scores, k):\n",
    "    threshold = np.sort(y_scores)[::-1][int(k*len(y_scores))]\n",
    "    y_pred = np.asarray([1 if i >= threshold else 0 for i in y_scores])\n",
    "    return metrics.precision_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magicloop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_hyper_params():\n",
    "    clfs = {\n",
    "        'RF': RandomForestClassifier(n_estimators=100),\n",
    "        'ET': ExtraTreesClassifier(n_estimators=10, criterion='entropy'),\n",
    "        'AB': AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), algorithm=\"SAMME\", n_estimators=200),\n",
    "        'LR': LogisticRegression(penalty='l2', C=1e5, solver='liblinear'),\n",
    "        'SVM': svm.SVC(kernel='linear', probability=True, random_state=0),\n",
    "        'GB': GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=10),\n",
    "        'NB': GaussianNB(),\n",
    "        'DT': DecisionTreeClassifier(),\n",
    "        'SGD': SGDClassifier(loss=\"hinge\", penalty=\"l2\"),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=3) \n",
    "            }\n",
    "\n",
    "    grid = { \n",
    "        'RF':{'n_estimators': [1,10,100,1000,10000], 'max_depth': [1,5,10,20,50,100], 'max_features': ['sqrt','log2'],'min_samples_split': [2,5,10]},\n",
    "        'LR': { 'penalty': ['l1','l2'], 'C': [0.00001,0.0001,0.001,0.01,0.1,1,10]},\n",
    "        'SGD': { 'loss': ['hinge','log','perceptron'], 'penalty': ['l2','l1','elasticnet']},\n",
    "        'ET': { 'n_estimators': [1,10,100,1000,10000], 'criterion' : ['gini', 'entropy'] ,'max_depth': [1,5,10,20,50,100], 'max_features': ['sqrt','log2'],'min_samples_split': [2,5,10]},\n",
    "        'AB': { 'algorithm': ['SAMME', 'SAMME.R'], 'n_estimators': [1,10,100,1000,10000]},\n",
    "        'GB': {'n_estimators': [1,10,100,1000,10000], 'learning_rate' : [0.001,0.01,0.05,0.1,0.5],'subsample' : [0.1,0.5,1.0], 'max_depth': [1,3,5,10,20,50,100]},\n",
    "        'NB' : {},\n",
    "        'DT': {'criterion': ['gini', 'entropy'], 'max_depth': [1,5,10,20,50,100], 'max_features': ['sqrt','log2'],'min_samples_split': [2,5,10]},\n",
    "        'SVM' :{'C' :[0.00001,0.0001,0.001,0.01,0.1,1,10],'kernel':['linear', 'rbf']},\n",
    "        'KNN' :{'n_neighbors': [1,5,10,25,50,100],'weights': ['uniform','distance'],'algorithm': ['auto','ball_tree','kd_tree']}\n",
    "           }\n",
    "\n",
    "    return clfs, grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magic_loop(models_to_run, clfs, grid, X, y, search = 1):\n",
    "    for n in range(1, 2):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "        for index, clf in enumerate([clfs[x] for x in models_to_run]):\n",
    "            print(models_to_run[index])\n",
    "            parameter_values = grid[models_to_run[index]]\n",
    "            try:\n",
    "                if(search):\n",
    "                    gs = GridSearchCV(clf, parameter_values, cv=5)\n",
    "                    start = time()\n",
    "                    y_pred_probs = gs.fit(X_train, y_train).predict_proba(X_test)[:,1]\n",
    "                    y_score = gs.fit(X_train, y_train).decision_function(X_test)\n",
    "                    print(precision_at_k(y_test, y_pred_probs, 0.05))\n",
    "                    print(\"GridSearch time: \" + (str)(time() - start))\n",
    "\n",
    "                else:\n",
    "                    start = time()\n",
    "                    rs = RandomizedSearchCV(clf, parameter_values, cv=5)\n",
    "                    y_pred_probs = rs.fit(X_train, y_train).predict_proba(X_test)[:,1]\n",
    "                    print(precision_at_k(y_test, y_pred_probs, 0.05)) \n",
    "                    print(\"RandomizedSearch time: \" + (str)(time() - start))\n",
    "            except IndexError as e:\n",
    "                print('Error:', e)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Titanic DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_sql_query(\"SELECT * from Titanic\", sqlite3.connect(\"Titanic.db\"))\n",
    "titanic = titanic.fillna(value=np.nan)\n",
    "lep = preprocessing.LabelEncoder()\n",
    "for ftr in list(titanic):\n",
    "    titanic[ftr] = lep.fit_transform(titanic[ftr].astype(str))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs, grid = define_hyper_params()\n",
    "models = ['KNN','RF','LR','ET','AB','GB','DT']\n",
    "features = ['age', 'Class', 'Ticket', 'pounds', 'Group', 'Ship', 'Joined', 'Job', 'Boat', 'sex', 'boat_location']\n",
    "X = titanic[features]\n",
    "y = titanic['survived'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magic Lopp with GridSearch and RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "0.8549618320610687\n",
      "RandomizedSearch time: 0.6100349426269531\n",
      "RF\n",
      "1.0\n",
      "RandomizedSearch time: 99.84871101379395\n",
      "LR\n",
      "1.0\n",
      "RandomizedSearch time: 1.2810733318328857\n",
      "ET\n",
      "1.0\n",
      "RandomizedSearch time: 135.86377096176147\n",
      "AB\n",
      "1.0\n",
      "RandomizedSearch time: 0.19901132583618164\n",
      "GB\n",
      "1.0\n",
      "RandomizedSearch time: 13.362764358520508\n",
      "DT\n",
      "1.0\n",
      "RandomizedSearch time: 0.16200900077819824\n"
     ]
    }
   ],
   "source": [
    "# GridSearch\n",
    "#magic_loop(models, clfs, grid, X, y, search = 1)\n",
    "\n",
    "#RandomSearch\n",
    "magic_loop(models, clfs, grid, X, y, search = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
