{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# SÃ³lo informe sobre errores\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_raw = 'He is the king . The king is royal . She is the royal  queen '\n",
    "\n",
    "# convert to lower case\n",
    "corpus_raw = corpus_raw.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for word in corpus_raw.split():\n",
    "    if word != '.': # because we don't want to treat . as a word\n",
    "        words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words=['he', 'the', 'she', 'king', 'queen', 'royal', 'is'] # words=set(words)\n",
    "word2int = {}\n",
    "int2word = {}\n",
    "vocab_size = len(unique_words) # gives the total number of unique words\n",
    "for i,word in enumerate(unique_words):\n",
    "    word2int[word] = i\n",
    "    int2word[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(word2int['queen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen\n"
     ]
    }
   ],
   "source": [
    "print(int2word[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw sentences is a list of sentences.\n",
    "raw_sentences = corpus_raw.split('.')\n",
    "sentences = []\n",
    "for sentence in raw_sentences:\n",
    "    sentences.append(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'is', 'the', 'king'], ['the', 'king', 'is', 'royal'], ['she', 'is', 'the', 'royal', 'queen']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "WINDOW_SIZE = 2\n",
    "for sentence in sentences:\n",
    "    for word_index, word in enumerate(sentence):\n",
    "        for nb_word in sentence[max(word_index - WINDOW_SIZE, 0) : min(word_index + WINDOW_SIZE, len(sentence)) + 1] : \n",
    "            if nb_word != word:\n",
    "                data.append([word, nb_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'is'], ['he', 'the'], ['is', 'he'], ['is', 'the'], ['is', 'king'], ['the', 'he'], ['the', 'is'], ['the', 'king'], ['king', 'is'], ['king', 'the'], ['the', 'king'], ['the', 'is'], ['king', 'the'], ['king', 'is'], ['king', 'royal'], ['is', 'the'], ['is', 'king'], ['is', 'royal'], ['royal', 'king'], ['royal', 'is'], ['she', 'is'], ['she', 'the'], ['is', 'she'], ['is', 'the'], ['is', 'royal'], ['the', 'she'], ['the', 'is'], ['the', 'royal'], ['the', 'queen'], ['royal', 'is'], ['royal', 'the'], ['royal', 'queen'], ['queen', 'the'], ['queen', 'royal']]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert numbers to one hot vectors\n",
    "def to_one_hot(data_point_index, vocab_size):\n",
    "    temp = np.zeros(vocab_size)\n",
    "    temp[data_point_index] = 1\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [] # input word\n",
    "y_train = [] # output word\n",
    "for data_word in data:\n",
    "    x_train.append(to_one_hot(word2int[ data_word[0] ], vocab_size))\n",
    "    y_train.append(to_one_hot(word2int[ data_word[1] ], vocab_size))\n",
    "# convert them to numpy arrays\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 7) (34, 7)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(1234567)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making placeholders for x_train and y_train\n",
    "x = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "y_label = tf.placeholder(tf.float32, shape=(None, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 5 # you can choose your own number\n",
    "W1 = tf.Variable(tf.random_normal([vocab_size, EMBEDDING_DIM]))\n",
    "b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM])) #bias\n",
    "hidden_representation = tf.add(tf.matmul(x,W1), b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, vocab_size]))\n",
    "b2 = tf.Variable(tf.random_normal([vocab_size]))\n",
    "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_representation, W2), b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is :  4.7867684\n",
      "loss is :  1.3295707\n",
      "loss is :  1.323726\n",
      "loss is :  1.3223268\n",
      "loss is :  1.3217282\n",
      "loss is :  1.3214027\n",
      "loss is :  1.3212001\n",
      "loss is :  1.3210629\n",
      "loss is :  1.3209642\n",
      "loss is :  1.32089\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) #make sure you do this!\n",
    "# define the loss function:\n",
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), reduction_indices=[1]))\n",
    "# define the training step:\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy_loss)\n",
    "n_iters = 10000\n",
    "# train for n_iter iterations\n",
    "for i in range(n_iters):\n",
    "    sess.run(train_step, feed_dict={x: x_train, y_label: y_train})\n",
    "    if i%1000==0:\n",
    "        print('loss is : ', sess.run(cross_entropy_loss, feed_dict={x: x_train, y_label: y_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.9350005e-01 -2.9161644e-01  5.4734558e-01  2.1402080e+00\n",
      "  -5.8095735e-01]\n",
      " [ 3.7450221e-01  7.6808488e-01  1.8853203e+00 -1.3790158e+00\n",
      "   1.9629668e-02]\n",
      " [ 5.5060935e-01  3.2977577e-02 -7.3554061e-02  2.3815403e+00\n",
      "  -1.7751952e+00]\n",
      " [ 6.4877892e-01 -1.3077191e+00  1.7724770e+00  1.6218843e+00\n",
      "   1.3255827e+00]\n",
      " [-5.2866977e-02 -2.1954162e+00 -2.2563904e-03  4.9293742e-01\n",
      "   1.3110733e+00]\n",
      " [ 2.1140649e+00  3.6594623e-01 -2.1624017e-01  7.4250340e-01\n",
      "  -1.2149994e+00]\n",
      " [-6.0443330e-01 -5.0912827e-02 -1.3596379e+00 -4.0226865e-01\n",
      "   1.9367465e+00]]\n",
      "----------\n",
      "[ 0.519766    0.45299616 -0.41997477  0.26156208  0.90566784]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(W1))\n",
    "print('----------')\n",
    "print(sess.run(b1))\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.0132661   0.16137972  0.1273708   2.40177     0.3247105 ]\n",
      " [ 0.89426816  1.221081    1.4653455  -1.1174537   0.9252975 ]\n",
      " [ 1.0703753   0.48597375 -0.49352884  2.6431024  -0.8695274 ]\n",
      " [ 1.1685449  -0.854723    1.3525022   1.8834465   2.2312505 ]\n",
      " [ 0.466899   -1.7424201  -0.42223117  0.7544995   2.216741  ]\n",
      " [ 2.633831    0.8189424  -0.636215    1.0040655  -0.3093316 ]\n",
      " [-0.08466733  0.40208334 -1.7796127  -0.14070657  2.8424144 ]]\n"
     ]
    }
   ],
   "source": [
    "vectors = sess.run(W1 + b1)\n",
    "\n",
    "# if you work it out, you will see that it has the same effect as running the node hidden representation\n",
    "print(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.466899   -1.7424201  -0.42223117  0.7544995   2.216741  ]\n"
     ]
    }
   ],
   "source": [
    "print(vectors[ word2int['queen'] ])\n",
    "# say here word2int['queen'] is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(vec1, vec2):\n",
    "    return np.sqrt(np.sum((vec1-vec2)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(word_index, vectors):\n",
    "    min_dist = 10000 # to act like positive infinity\n",
    "    min_index = -1\n",
    "    query_vector = vectors[word_index]\n",
    "    for index, vector in enumerate(vectors):\n",
    "        if euclidean_dist(vector, query_vector) < min_dist and not np.array_equal(vector, query_vector):\n",
    "            min_dist = euclidean_dist(vector, query_vector)\n",
    "            min_index = index\n",
    "    return min_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen\n",
      "king\n",
      "she\n"
     ]
    }
   ],
   "source": [
    "print(int2word[find_closest(word2int['king'], vectors)])\n",
    "print(int2word[find_closest(word2int['queen'], vectors)])\n",
    "print(int2word[find_closest(word2int['royal'], vectors)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(vec1, vec2):\n",
    "    \"\"\"Takes 2 vectors a, b and returns the cosine similarity according \n",
    "    to the definition of the dot product\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_a = np.linalg.norm(vec1)\n",
    "    norm_b = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(word_index, vectors):\n",
    "    max_dist = -1 # to act like positive infinity\n",
    "    max_index = -1\n",
    "    query_vector = vectors[word_index]\n",
    "    for index, vector in enumerate(vectors):\n",
    "        if cos_sim(vector, query_vector) > max_dist and not np.array_equal(vector, query_vector):\n",
    "            max_dist = cos_sim(vector, query_vector)\n",
    "            max_index = index\n",
    "    return max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen\n",
      "king\n",
      "she\n"
     ]
    }
   ],
   "source": [
    "print(int2word[find_closest(word2int['king'], vectors)])\n",
    "print(int2word[find_closest(word2int['queen'], vectors)])\n",
    "print(int2word[find_closest(word2int['royal'], vectors)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "np.set_printoptions(suppress=True)\n",
    "vectors = model.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "normalizer = preprocessing.Normalizer()\n",
    "vectors =  normalizer.fit_transform(vectors, 'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he 0.9991169 0.042017374\n",
      "is 0.00024354966 -0.99999994\n",
      "the 0.7357115 -0.67729497\n",
      "king -0.5410816 0.8409701\n",
      "the 0.7357115 -0.67729497\n",
      "king -0.5410816 0.8409701\n",
      "is 0.00024354966 -0.99999994\n",
      "royal -0.8735178 -0.48679233\n",
      "she 0.88298255 -0.4694059\n",
      "is 0.00024354966 -0.99999994\n",
      "the 0.7357115 -0.67729497\n",
      "royal -0.8735178 -0.48679233\n",
      "queen 0.60694957 0.7947404\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(vectors[:,0],vectors[:,1])\n",
    "for word in words:\n",
    "    print(word, vectors[word2int[word]][0], vectors[word2int[word]][1])\n",
    "    ax.annotate(word, (vectors[word2int[word]][0],vectors[word2int[word]][1] ))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.6185790300369263)]\n",
      "[('italy', 0.5761076211929321)]\n"
     ]
    }
   ],
   "source": [
    "filename = 'vectors.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)\n",
    "result = model.most_similar(positive=['rome', 'france'], negative=['paris'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
